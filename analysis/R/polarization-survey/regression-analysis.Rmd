---
title: "Regression Analysis"
output:
  pdf_document: default
  html_document: default
---

```{r include = FALSE}
library(lmerTest)
library(lme4)
library(report)
```

```{r data}
survey_data <- read.csv('../../../backend/data/database/survey_data.csv')
survey_data$TreatmentGroup <- as.factor(survey_data$TreatmentGroup)
survey_data$TreatmentGroup <- relevel(survey_data$TreatmentGroup, ref = "machine")
head(survey_data, n = 10)
```

### General Result (all treatments)

```{r General Results}
m_general = glmer(TreatedIsLessPolar ~ TreatmentGroup + (1 | FK_ParticipantId),
              data=survey_data, family = "binomial")
summary(m_general)
report(m_general)
```

## **Model Interpretation**

### **Model Overview**

-   **Dependent Variable (TreatedIsLessPolar)**: A binary indicator of whether the treated text is perceived as less polarized than the original text.
-   **Predictor (TreatmentGroup)**: Three treatment groups: **machine paraphrasing** (reference category), **human paraphrasing**, and **placebo**.
-   **Random Effects**:
    -   A random intercept for each participant (FK_ParticipantId) accounts for individual variability in polarization perceptions.

------------------------------------------------------------------------

### **Key Metrics**

1.  **AIC**: 296.3, **BIC**: 311.0, **Log-Likelihood**: -144.2, **Deviance**: 288.3, **df.resid**: 288
    -   Lower AIC and BIC values indicate a better model fit relative to alternative models.
2.  **Conditional R²**: 0.47, representing the variance explained by both fixed and random effects.
3.  **Marginal R²**: 0.33, representing the variance explained by the fixed effects alone.

------------------------------------------------------------------------

### **Random Effects**

-   **Variance of Participant-Level Random Intercept**: 0.8849, with a standard deviation of 0.9407.
    -   This indicates moderate variability in participants' baseline differences in the perception of polarization between original and treated texts.

------------------------------------------------------------------------

### **Fixed Effects**

#### **1. Intercept:**

-   **Estimate**: 1.3956
-   **Interpretation**: The mean log-odds of a treated text being perceived as less polarized compared to the original text, when the treatment is **machine paraphrasing** (reference category), is **1.40**.
    -   This positive value indicates a higher likelihood of the treated text being seen as less polarized than the original.
-   **Significance**: Highly significant (p \< 0.001).

#### **2. TreatmentGrouphuman:**

-   **Estimate**: 0.9753
-   **Interpretation**: Compared to **machine paraphrasing**, the log-odds of a treated text being seen as less polarized when the treatment is **human paraphrasing** increase by **0.98**.
    -   This effect is **not statistically significant** (p = 0.057), suggesting a positive but marginally non-significant difference between human and machine paraphrasing.

#### **3. TreatmentGroupplacebo:**

-   **Estimate**: -2.4357
-   **Interpretation**: Compared to **machine paraphrasing**, the log-odds of a treated text being seen as less polarized when the treatment is **placebo** decrease by **-2.44**.
    -   This effect is **highly significant** (p \< 0.001), indicating that the placebo treatment significantly decreases the likelihood of the treated text being perceived as less polarized.

------------------------------------------------------------------------

### **Confidence Intervals**

-   The **95% Confidence Interval** for each fixed effect provides the range of plausible values for the parameter estimates:
    -   Intercept: [0.72, 2.07] – consistently positive, indicating a strong likelihood that machine paraphrasing is seen as less polarized than the original.
    -   TreatmentGrouphuman: [-0.03, 1.98] – includes zero, confirming the non-significance of the effect.
    -   TreatmentGroupplacebo: [-3.39, -1.48] – consistently negative, confirming the strong negative effect of the placebo.

------------------------------------------------------------------------

### **Correlation of Fixed Effects**

-   The correlation between the intercept and **TreatmentGrouphuman** is -0.554, indicating a moderate negative relationship.
-   The correlation between the intercept and **TreatmentGroupplacebo** is -0.759, showing a stronger negative relationship.

------------------------------------------------------------------------

### **Summary of Findings**

1.  **Effectiveness of Treatments**:
    -   **Machine paraphrasing** is the reference category and shows a strong likelihood of being perceived as less polarized.
    -   **Human paraphrasing** marginally increases the likelihood of being perceived as less polarized compared to machine paraphrasing, but this effect is not statistically significant.
    -   **Placebo treatment** significantly decreases the likelihood of being perceived as less polarized.
2.  **Participant-Level Variability**:
    -   There is moderate variability in how participants perceive the reduction in polarization across different treatments.
3.  **Model Fit**:
    -   The model explains **33%** of the variance in polarization perceptions based on fixed effects alone (marginal R²), while it explains **47%** of the variance when accounting for both fixed and random effects (conditional R²).

### **RQ1** Can LLMs mitigate textual polarization in social media texts?

Logistic Regression for mitigation effect.

```{r RQ1}
machine_placebo <- subset(survey_data, TreatmentGroup %in% c("machine", "placebo"))
model_rq1 = glmer(TreatedIsLessPolar ~ TreatmentGroup + (1 | FK_ParticipantId),
              data=machine_placebo, family = "binomial")
summary(model_rq1)

report(model_rq1)
```

## **Model Interpretation**

### **Model Overview**

-   **Dependent Variable (TreatedIsLessPolar)**: A binary indicator of whether the treated text is perceived as less polarized than the original text.
-   **Predictor (TreatmentGroup)**: Two treatment groups: **machine paraphrasing** (reference category) and **placebo**.
-   **Random Effects**:
    -   A random intercept for each participant (FK_ParticipantId) accounts for individual variability in polarization perceptions.

------------------------------------------------------------------------

### **Key Metrics**

1.  **AIC**: 226.7, **BIC**: 236.6, **Log-Likelihood**: -110.4, **Deviance**: 220.7, **df.resid**: 193
    -   Lower AIC and BIC values indicate a better model fit relative to alternative models.
2.  **Conditional R²**: 0.42, representing the variance explained by both fixed and random effects.
3.  **Marginal R²**: 0.26, representing the variance explained by the fixed effects alone.

------------------------------------------------------------------------

### **Random Effects**

-   **Variance of Participant-Level Random Intercept**: 0.9159, with a standard deviation of 0.957.
    -   This indicates moderate variability in participants' baseline differences in the perception of polarization between original and treated texts.

------------------------------------------------------------------------

### **Fixed Effects**

#### **1. Intercept:**

-   **Estimate**: 1.4029
-   **Interpretation**: The mean log-odds of a treated text being perceived as less polarized compared to the original text, when the treatment is **machine paraphrasing** (reference category), is **1.40**.
    -   This positive value indicates a higher likelihood of the treated text being seen as less polarized than the original.
-   **Significance**: Highly significant (p \< 0.001).

#### **2. TreatmentGroupplacebo:**

-   **Estimate**: -2.4472
-   **Interpretation**: Compared to **machine paraphrasing**, the log-odds of a treated text being seen as less polarized when the treatment is **placebo** decrease by **-2.45**.
    -   This effect is **highly significant** (p \< 0.001), indicating that the placebo treatment significantly decreases the likelihood of the treated text being perceived as less polarized.

------------------------------------------------------------------------

### **Confidence Intervals**

-   The **95% Confidence Interval** for each fixed effect provides the range of plausible values for the parameter estimates:
    -   Intercept: [0.71, 2.10] – consistently positive, indicating a strong likelihood that machine paraphrasing is seen as less polarized than the original.
    -   TreatmentGroupplacebo: [-3.43, -1.46] – consistently negative, confirming the strong negative effect of the placebo.

------------------------------------------------------------------------

### **Correlation of Fixed Effects**

-   The correlation between the intercept and **TreatmentGroupplacebo** is -0.768, indicating a moderate negative relationship.

------------------------------------------------------------------------

### **Summary of Findings**

1.  **Effectiveness of Treatments**:
    -   **Machine paraphrasing** (reference category) shows a strong likelihood of being perceived as less polarized than the original text.
    -   **Placebo treatment** significantly decreases the likelihood of the treated text being perceived as less polarized compared to machine paraphrasing.
2.  **Participant-Level Variability**:
    -   There is moderate variability in how participants perceive the reduction in polarization across different treatments.
3.  **Model Fit**:
    -   The model explains **26%** of the variance in polarization perceptions based on fixed effects alone (marginal R²), while it explains **42%** of the variance when accounting for both fixed and random effects (conditional R²).

------------------------------------------------------------------------

## **RQ2** Can LLMs significantly reduce perceived polarization in social media texts?

```{r}
#model_rq2 <- lmer(DiffLikertTreatedOriginal ~ TreatmentGroup + TweetBias * ParticipantLeaning + 
#                  (1 | FK_ParticipantId), 
#                  data = machine_placebo)
model_rq2 <- lmer(DiffLikertTreatedOriginal ~ TreatmentGroup +(1 | FK_ParticipantId), 
                  data = machine_placebo)
summary(model_rq2)

report(model_rq2)
```

## **Model Interpretation**

### **Model Overview**

-   **Dependent Variable (DiffLikertTreatedOriginal)**: The difference in polarization scores between the treated and original texts, measured on a Likert scale.
-   **Predictor (TreatmentGroup)**: Two treatment groups: **machine paraphrasing** (reference category) and **placebo**.
-   **Random Effects**:
    -   A random intercept for each participant (FK_ParticipantId) accounts for individual variability in score differences.

------------------------------------------------------------------------

### **Key Metrics**

1.  **REML Criterion**: 613.6. A lower REML value suggests a better model fit when comparing similar models.
2.  **Residual Standard Deviation**: 1.0441, indicating the average deviation of observed values from predicted values after accounting for fixed and random effects.
3.  **R² Values**:
    -   **Conditional R²**: 0.46, representing the variance explained by both fixed and random effects.
    -   **Marginal R²**: 0.30, representing the variance explained by the fixed effects alone.

------------------------------------------------------------------------

### **Random Effects**

-   **Variance of Participant-Level Random Intercept**: 0.3284, with a standard deviation of 0.5731.
    -   This indicates moderate variability in participants' baseline differences in polarization scores.
-   **Residual Variance**: 1.0901, with a standard deviation of 1.0441.

------------------------------------------------------------------------

### **Fixed Effects**

#### **1. Intercept:**

-   **Estimate**: -1.7400
-   **Interpretation**: When the treatment group is **machine paraphrasing** (reference category), the mean difference in Likert scale polarization scores is **-1.74**.
    -   This negative value indicates that machine paraphrasing significantly reduces polarization scores compared to the original texts.
-   **Significance**: Highly significant (p \< 0.001).

#### **2. TreatmentGroupplacebo:**

-   **Estimate**: 1.5629
-   **Interpretation**: Compared to **machine paraphrasing**, the mean difference in polarization scores increases by **1.56** when the treatment is **placebo**.
    -   This effect is **highly significant** (p \< 0.001), indicating that the placebo treatment significantly increases the perception of polarization compared to machine paraphrasing.

------------------------------------------------------------------------

### **Confidence Intervals**

-   The **95% Confidence Interval** for each fixed effect provides the range of plausible values for the parameter estimates:
    -   Intercept: [-2.05, -1.43] – consistently negative, indicating a robust reduction in polarization scores for machine paraphrasing.
    -   TreatmentGroupplacebo: [1.13, 2.00] – consistently positive, confirming the strong effect of the placebo in increasing polarization.

------------------------------------------------------------------------

### **Correlation of Fixed Effects**

-   The correlation between the intercept and **TreatmentGroupplacebo** is -0.700, indicating a moderate negative relationship.

------------------------------------------------------------------------

### **Summary of Findings**

1.  **Effectiveness of Treatments**:
    -   **Machine paraphrasing** significantly reduces polarization scores, with a mean reduction of **1.74 points** on the Likert scale.
    -   **Placebo treatment** significantly increases the perception of polarization compared to machine paraphrasing, with a mean increase of **1.56 points**.
2.  **Participant-Level Variability**:
    -   There is moderate variability in baseline score differences across participants, as indicated by the random effects.
3.  **Model Fit**:
    -   Fixed effects explain **30%** of the variance in polarization score differences (marginal R²), while the full model explains **46%** (conditional R²), suggesting substantial explanatory power.

------------------------------------------------------------------------

## **RQ3** Can LLMs mitigate textual polarization as good as humans?

```{r}
# Compare LLM vs. Human
machine_human <- subset(survey_data, TreatmentGroup %in% c("machine", "human"))
model_rq3 <- lmer(DiffLikertTreatedOriginal ~ TreatmentGroup + (1 | FK_ParticipantId), 
                  data = machine_human)
summary(model_rq3)

report(model_rq3)
```

## **Model Interpretation**

### **Model Overview**

-   **Dependent Variable (DiffLikertTreatedOriginal)**: The difference in polarization scores between the treated and original texts, measured on a Likert scale.
-   **Predictor (TreatmentGroup)**: Two treatment groups: **machine paraphrasing** (reference category) and **human paraphrasing**.
-   **Random Effects**:
    -   A random intercept for each participant (FK_ParticipantId) accounts for individual variability in score differences.

------------------------------------------------------------------------

### **Key Metrics**

1.  **REML Criterion**: 713.9. A lower REML value suggests a better model fit when comparing similar models.
2.  **Residual Standard Deviation**: 1.4148, indicating the average deviation of observed values from predicted values after accounting for fixed and random effects.
3.  **R² Values**:
    -   **Conditional R²**: 0.12, representing the variance explained by both fixed and random effects.
    -   **Marginal R²**: 0.0039, representing the variance explained by the fixed effects alone.

------------------------------------------------------------------------

### **Random Effects**

-   **Variance of Participant-Level Random Intercept**: 0.2588, with a standard deviation of 0.5087.
    -   This indicates low variability in participants' baseline differences in polarization scores.
-   **Residual Variance**: 2.0017, with a standard deviation of 1.4148.

------------------------------------------------------------------------

### **Fixed Effects**

#### **1. Intercept:**

-   **Estimate**: -1.7400
-   **Interpretation**: When the treatment group is **machine paraphrasing** (reference category), the mean difference in Likert scale polarization scores is **-1.74**.
    -   This negative value indicates that machine paraphrasing significantly reduces polarization scores compared to the original texts.
-   **Significance**: Highly significant (p \< 0.001).

#### **2. TreatmentGrouphuman:**

-   **Estimate**: 0.1879
-   **Interpretation**: Compared to **machine paraphrasing**, the mean difference in polarization scores increases slightly (by **0.19**) when the treatment is **human paraphrasing**.
    -   However, this effect is **not statistically significant** (p = 0.451), suggesting no meaningful difference between machine and human paraphrasing in reducing polarization.

------------------------------------------------------------------------

### **Confidence Intervals**

-   The **95% Confidence Interval** for each fixed effect provides the range of plausible values for the parameter estimates:
    -   Intercept: [-2.08, -1.40] – consistently negative, indicating a robust reduction in polarization scores for machine paraphrasing.
    -   TreatmentGrouphuman: [-0.30, 0.68] – includes zero, confirming the non-significance of the effect.

------------------------------------------------------------------------

### **Correlation of Fixed Effects**

-   The correlation between the intercept and **TreatmentGrouphuman** is -0.700, indicating a moderate negative relationship.

------------------------------------------------------------------------

### **Summary of Findings**

1.  **Effectiveness of Treatments**:
    -   **Machine paraphrasing** significantly reduces polarization scores, with a mean reduction of **1.74 points** on the Likert scale.
    -   **Human paraphrasing** does not significantly reduce polarization scores compared to machine paraphrasing, with a non-significant mean increase of **0.19 points**.
2.  **Participant-Level Variability**:
    -   There is low variability in baseline score differences across participants, as indicated by the random effects.
3.  **Model Fit**:
    -   Fixed effects explain only **0.39%** of the variance in polarization score differences (marginal R²), while the full model explains **12%** (conditional R²), suggesting weak explanatory power.

------------------------------------------------------------------------

## **RQ4** Does political bias influence the participants' perception of textual polarization?

```{r}
model_rq4 <- lmer(OriginalLikertValue ~ TweetBias * ParticipantLeaning + 
                  (1 | FK_ParticipantId), data = survey_data)
summary(model_rq4)

report(model_rq4)
```

## **Model Interpretation**

### **Model Overview**

-   **Dependent Variable (OriginalLikertValue)**: The Likert scale value representing the polarization of the tweet as assessed by participants.
-   **Predictors**:
    -   **TweetBias** (Right vs. Left bias in the tweet).
    -   **ParticipantLeaning** (center-left, center-right, far-left, far-right, left, right, not informed).
    -   Interaction between **TweetBias** and **ParticipantLeaning**.
-   **Random Effects**: A random intercept for each participant (FK_ParticipantId) is included to account for individual variability in polarization scores.

------------------------------------------------------------------------

### **Key Metrics**

1.  **REML Criterion**: 791.5. This value indicates model fit, with lower values suggesting better fit when comparing similar models.
2.  **Residual Standard Deviation**: 0.9012, showing the average deviation of the observed values from the predicted values after accounting for fixed and random effects.
3.  **R² Values**:
    -   **Conditional R²**: 0.18, indicating that 18% of the variance in the polarization scores is explained by both fixed and random effects.
    -   **Marginal R²**: 0.08, suggesting that the fixed effects alone explain 8% of the variance.

------------------------------------------------------------------------

### **Random Effects**

-   **Variance of Participant-Level Random Intercept**: 0.09907 (SD = 0.3148), indicating small variability in baseline differences between participants' polarization scores.
-   **Residual Variance**: 0.81209 (SD = 0.9012), reflecting the unexplained variability after accounting for fixed and random effects.

------------------------------------------------------------------------

### **Fixed Effects**

#### **1. Intercept**:

-   **Estimate**: 4.188 (95% CI [3.69, 4.68]).
-   **Interpretation**: When the tweet is left-biased and the participant leans center, the average polarization score is **4.19**, indicating a relatively neutral to moderately polarized tweet.
-   **Significance**: Highly significant (p \< 0.001).

#### **2. Main Effects**:

-   **TweetBias [Right]**:
    -   **Estimate**: 0.0625 (95% CI [-0.56, 0.69]).
    -   **Interpretation**: The effect of tweet bias being right-wing is **non-significant** (p = 0.845), suggesting no difference in polarization between right- and left-biased tweets in general.
-   **ParticipantLeaning**:
    -   **Center-left**: **Non-significant** (p = 0.474), suggesting no substantial difference in polarization compared to the center group.
    -   **Center-right**: **Non-significant** (p = 0.630), similarly showing no significant difference.
    -   **Far-left**: **Significant negative effect** (beta = -1.19, p = 0.035), indicating that far-left participants perceive a significantly lower level of polarization compared to center participants.
    -   **Far-right**: **Non-significant** (p = 0.283), showing no significant difference in polarization perception.
    -   **Left**: **Non-significant** (p = 0.872), suggesting no effect.
    -   **Not informed**: **Non-significant** (p = 0.601), with a slight positive effect, but not enough to be meaningful.
    -   **Right**: **Non-significant** (p = 0.479), showing no substantial difference.

#### **3. Interaction Effects (TweetBias × ParticipantLeaning)**:

-   **Far-left × Right-Bias**:
    -   **Estimate**: 1.687 (95% CI [0.28, 3.09]).
    -   **Interpretation**: Far-left participants perceive a **significantly higher level of polarization** when exposed to right-biased tweets (p = 0.019).
-   **Far-right × Right-Bias**:
    -   **Estimate**: -2.063 (95% CI [-3.94, -0.18]).
    -   **Interpretation**: Far-right participants perceive a **significantly lower level of polarization** when exposed to right-biased tweets (p = 0.032).
-   **Other Interactions** (e.g., **center-left**, **center-right**, etc.): All non-significant, suggesting no meaningful interaction between tweet bias and these participant leanings.

------------------------------------------------------------------------

### **Confidence Intervals and p-Values**

-   95% Confidence Intervals (CIs) provide the range of plausible values for each parameter:
    -   The **far-left × Right-bias** interaction has a positive and significant effect, with the 95% CI not including zero.
    -   The **far-right × Right-bias** interaction is also significant, with a negative effect and the 95% CI not including zero.

------------------------------------------------------------------------

### **Summary of Findings**

1.  **Tweet Bias**:
    -   The bias of the tweet (left vs. right) alone does not significantly influence the polarization score (p = 0.845).
2.  **Participant Leaning**:
    -   Participants with far-left political leanings perceive a significantly lower level of polarization, while other groups (center-left, center-right, far-right, left, right, and not informed) show no significant effects.
3.  **Interaction Effects**:
    -   **Far-left participants** perceive a significantly **higher polarization** in right-biased tweets.
    -   **Far-right participants** perceive a significantly **lower polarization** in right-biased tweets.
4.  **Model Fit**:
    -   The model explains a moderate portion of the variance (18% total), with fixed effects alone explaining 8%.

------------------------------------------------------------------------

### Textual Cohesion

```{r}
survey_data$IsCoherent <- ifelse(survey_data$AnswerQ3 > 3, 1, 0)
model_cohesion <- lmer(IsCoherent ~ TreatmentGroup + (1 | FK_ParticipantId),
                       data=survey_data)
summary(model_cohesion)

report(model_cohesion)
```

## Linear Mixed Model Results: Predicting **IsCoherent** with **TreatmentGroup**

### Model Formula: IsCoherent \~ TreatmentGroup + (1 \| FK_ParticipantId)

-   **Random Effect**: **FK_ParticipantId** (participant-level random intercept).
-   **Fixed Effect**: **TreatmentGroup** as a predictor of **IsCoherent**.

### Model Fit:

-   **REML criterion at convergence**: 270.1
-   **Scaled Residuals**:
    -   Min: -2.40, 1st Quartile: 0.13, Median: 0.31, 3rd Quartile: 0.46, Max: 1.43
-   **Number of Observations**: 292
-   **Number of Groups**: 73 participants

### Random Effects:

-   **Participant-Level Intercept**:
    -   Variance: 0.02657
    -   Std. Dev.: 0.163
-   **Residual**:
    -   Variance: 0.12215
    -   Std. Dev.: 0.3495

### Fixed Effects:

1.  **Intercept (Machine Group)**:
    -   **Estimate**: 0.80
    -   **95% CI**: [0.71, 0.89]
    -   **t-value**: 16.74, **p** \< 0.001
    -   Interpretation: The baseline coherence score for the **machine** group is statistically significant.
2.  **Human Treatment**:
    -   **Estimate**: -0.08
    -   **95% CI**: [-0.22, 0.05]
    -   **t-value**: -1.19, **p** = 0.238
    -   Interpretation: No significant difference between **human** and **machine** groups.
3.  **Placebo Treatment**:
    -   **Estimate**: 0.12
    -   **95% CI**: [-0.02, 0.25]
    -   **t-value**: 1.71, **p** = 0.089
    -   Interpretation: The **placebo treatment** shows a weak positive effect, but it is not statistically significant at the 0.05 level.

### Model's R²:

-   **Conditional R²** (total explanatory power): 0.21
-   **Marginal R²** (fixed effects alone): 0.04

### Summary Interpretation:

-   The **machine group** has a significantly higher baseline coherence score than the **human** and **placebo** groups.

-   Both **human** and **placebo** treatments do not show strong evidence of affecting coherence, though the placebo treatment suggests a slight positive trend (p = 0.092).
